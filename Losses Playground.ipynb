{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e9868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c56aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from data_handlers import YCDataset, SampleBatchIdx\n",
    "from models import EmbeddingsMapping\n",
    "from utils import compute_normalization_parameters\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, log, exp\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "opj = lambda x, y: os.path.join(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4646ccd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1193, 10), (417, 9))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_csv('training_with_labels.csv')\n",
    "validation_df = pd.read_csv('validation_with_labels.csv')\n",
    "training_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad29b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YCDataset(training_df, video_len=775)\n",
    "batch_sampler = SampleBatchIdx(train_dataset, 8, 24)\n",
    "train_dl = DataLoader(train_dataset, batch_sampler = batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065e521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "model = EmbeddingsMapping(512, video_layers=2, text_layers=2, drop_layers=2, learnable_drop=True, normalization_dataset=train_dataset)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb52f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_nce(features_1, features_2, correspondance_mat, eps=1e-8, gamma=1, hard_ratio=1):\n",
    "    corresp = correspondance_mat.to(torch.float32)\n",
    "    prod = (features_1 @ features_2.T) / gamma\n",
    "    \n",
    "    prod_exp = exp(prod - prod.max(dim=1, keepdim=True).values)\n",
    "    nominator = (prod_exp * corresp).sum(dim=1) # this sum needs to maximized?\n",
    "    denominator = prod_exp.sum(dim=1)\n",
    "    \n",
    "    nll = -log(nominator / (denominator + eps)) # minimize this ratio will give spread to the data?\n",
    "    \n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "def compute_clust_loss(sf, step_len, ff, video_len, dif, frame_gamma=10, l2_normalize=False):\n",
    "    all_pooled_frames = []\n",
    "    all_step_features = []\n",
    "    frame_labels = [0]\n",
    "    for idx, sample in enumerate(zip(sf, step_len, ff, video_len, dif)):\n",
    "        st, s_l, fr, v_l, dis = sample\n",
    "        st, fr = torch.vstack([st[:s_l], dis]), fr[:v_l] # appending distractor to step as a frame can also be correlated to be dropped\n",
    "        \n",
    "        if l2_normalize:\n",
    "            st = F.normalize(st, p=2, dim=1)\n",
    "            fr = F.normalize(fr, p=2, dim=1)\n",
    "            frame_gamma = 0.1\n",
    "        \n",
    "        sim = (st @ fr.T) # similarity comparison between steps and frames\n",
    "        # TODO: check to see if some kind of attention can be learned here\n",
    "        weights = F.softmax(sim / frame_gamma, dim=1) # this gamma allows expanded attention of steps to all frames -> temperature\n",
    "        attended_st = weights @ fr\n",
    "        all_pooled_frames.append(attended_st)\n",
    "        frame_labels.append(s_l + 1)\n",
    "        all_step_features.append(st)\n",
    "    all_pooled_frames = torch.cat(all_pooled_frames, dim=0)\n",
    "    all_step_features = torch.cat(all_step_features, dim=0)\n",
    "    N_steps = all_pooled_frames.shape[0]\n",
    "    frame_labels = np.cumsum(frame_labels)\n",
    "    xz_label_mat = torch.zeros([N_steps, N_steps])\n",
    "    for i in range(1, len(frame_labels)):\n",
    "        xz_label_mat[frame_labels[i-1]:frame_labels[i], frame_labels[i-1]:frame_labels[i]] = 1.\n",
    "    xz_label_mat = xz_label_mat.to(device)\n",
    "    \n",
    "    xz_loss = mil_nce(all_pooled_frames, all_step_features, xz_label_mat)\n",
    "    return xz_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cec2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_costs(sample, l2_normalize, gamma_xz, drop_cost_type, keep_percentile):\n",
    "    sf, step_len, ff, video_len, dis = sample\n",
    "    sf, ff = sf[:step_len], ff[:video_len]\n",
    "    \n",
    "    if l2_normalize:\n",
    "        sf = F.normalize(sf, p=2, dim=1)\n",
    "        ff = F.normalize(ff, p=2, dim=1)\n",
    "    sim = sf @ ff.T # getting similarity costs\n",
    "    if drop_cost_type == 'logit':\n",
    "        k = max([1, int(torch.numel(sim) * keep_percentile)])\n",
    "        baseline_logit = torch.topk(sim.reshape(-1), k).values[-1].detach()\n",
    "        baseline_logits = baseline_logit.repeat([1, sim.shape[1]])\n",
    "        sims_ext = torch.cat([sim, baseline_logits], dim=0)\n",
    "    else:\n",
    "        if l2_normalize:\n",
    "            dis = F.normalize()\n",
    "        distractor_sim = ff @ dis\n",
    "        sims_ext = torch.cat([sim, distractor_sim[None, :]], dim=0)\n",
    "\n",
    "    softmax_sims = F.softmax(sims_ext/gamma_xz, dim=0) \n",
    "    matching_probs, drop_probs = softmax_sims[:-1], softmax_sims[-1]\n",
    "    zx_costs = - log(matching_probs + 1e-5)\n",
    "    drop_costs = - log(drop_probs + 1e-5)  \n",
    "    return zx_costs, drop_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "397fe748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alignment_loss(samples, l2_normalize, gamma_xz, drop_cost_type, keep_percentile):\n",
    "    \n",
    "    gamma_xz = 0.1 if l2_normalize else gamma_xz\n",
    "    sf, step_len, ff, video_len, dif = samples\n",
    "\n",
    "    zx_costs_list = []\n",
    "    drop_costs_list = []\n",
    "    print(len(samples))\n",
    "    \n",
    "    for idx, sample in enumerate(zip(sf, step_len, ff, video_len, dif)):\n",
    "        zx_costs, drop_costs = compute_all_costs(sample, l2_normalize=False, gamma_xz=10, drop_cost_type=drop_cost_type, keep_percentile=keep_percentile)\n",
    "        zx_costs_list.append(zx_costs)\n",
    "        drop_costs_list.append(drop_costs)\n",
    "        break\n",
    "        \n",
    "    return zx_costs_list, drop_costs_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dbe046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([6, 90])\n",
      "540 torch.Size([6, 90])\n",
      "162\n",
      "tensor(-8.0052, device='cuda:3')\n",
      "tensor([[-8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052, -8.0052,\n",
      "         -8.0052, -8.0052]], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "def run_model(vf, sf, distractor):\n",
    "    frame_features = model.map_video(vf)\n",
    "    step_features = model.map_text(sf)\n",
    "    distractor_features = model.compute_distractors(distractor)\n",
    "\n",
    "    return frame_features, step_features, distractor_features\n",
    "\n",
    "frame_gamma = 10\n",
    "gamma_xz = 10\n",
    "keep_percentile = 1\n",
    "drop_cost_type = \n",
    "l2_normalize = False\n",
    "for batch in train_dl:\n",
    "    step_len, step_features, video_len, video_features = batch['step_len'], batch['step_feature'], batch['video_len'], batch['video_feature']\n",
    "    distractors = torch.stack([ s[:size].mean(0) for s, size in zip(step_features, step_len)], dim=0) # also taking care of the distractor padding (dont worry about it later)\n",
    "    ff, sf, dif = run_model(video_features.to(device), step_features.to(device), distractors.to(device))\n",
    "#     loss = compute_clust_loss(sf, step_len, ff, video_len, dif, frame_gamma, l2_normalize)\n",
    "    loss = compute_alignment_loss((sf, step_len, ff, video_len, dif), l2_normalize, gamma_xz, keep_percentile) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed22dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
