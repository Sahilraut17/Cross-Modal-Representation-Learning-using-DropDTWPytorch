{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa048414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbd2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from data_handlers import YCDataset, SampleBatchIdx\n",
    "from models import EmbeddingsMapping\n",
    "from losses import compute_all_costs, compute_clust_loss, compute_alignment_loss\n",
    "from dtw import drop_dtw\n",
    "from utils import compute_normalization_parameters\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, log, exp\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "opj = lambda x, y: os.path.join(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a0a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('training_with_labels_s3dg.csv')\n",
    "validation_df = pd.read_csv('validation_with_labels_s3dg.csv')\n",
    "\n",
    "gt_training = torch.load('s3d_labelled_video_train.pkl')\n",
    "gt_validation = torch.load('s3d_labelled_video_val.pkl')\n",
    "# validation_df.head()\n",
    "# print(gt_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62caeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb22dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataset = YCDataset(training_df, video_len=775)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "valid_dataset = YCDataset(validation_df, video_len=775)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351b9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:1'\n",
    "# folder_name = Path('ckpts_notext_s3dg_cl4_dtw2.5_learn_adamw_batchnorm_dropout_lr_1e-5')\n",
    "# state_dict = torch.load(folder_name/'best_model_state_20.pth')\n",
    "# model = EmbeddingsMapping(512, video_layers=3, text_layers=0, drop_layers=2, learnable_drop=True, normalization_dataset=train_dataset, batch_norm=True)\n",
    "# model.load_state_dict(state_dict['model'])\n",
    "# model = model.to(device)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8215f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sim(x, z):\n",
    "    return x @ z.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1a9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_eval(vf, sf, distractor, drop_cost_type):\n",
    "    with torch.no_grad():\n",
    "        frame_features = model.map_video(vf)\n",
    "        step_features = model.map_text(sf)\n",
    "        if drop_cost_type == 'learn':\n",
    "            distractor_features = model.compute_distractors(distractor)\n",
    "        else:\n",
    "            distractor_features = [None] * frame_features.shape[0]\n",
    "        return step_features, frame_features, distractor_features\n",
    "    \n",
    "def recall_acc(frame_assignment, gt_assignment):\n",
    "    return ((frame_assignment == gt_assignment).sum())/ gt_assignment.size\n",
    "\n",
    "def framewise_accuracy(frame_assignment, gt_assignment, num_frames, use_unlabeled=False):\n",
    "    # to discount unlabeled frames in gt\n",
    "    if not use_unlabeled:\n",
    "        unlabled = np.count_nonzero(gt_assignment == -1)\n",
    "        num_frames = num_frames - unlabled\n",
    "        fa = np.logical_and(frame_assignment == gt_assignment, gt_assignment != -1).sum()\n",
    "    else:\n",
    "        fa = np.count_nonzero((frame_assignment == gt_assignment))\n",
    "    # framewise accuracy\n",
    "    fa = fa / num_frames if num_frames != 0 else 0\n",
    "    return fa\n",
    "\n",
    "def IoU(frame_assignment, gt_assignment, num_steps):\n",
    "\n",
    "    intersection, union = 0, 0\n",
    "    for s in range(num_steps):\n",
    "        intersection += np.logical_and(gt_assignment == s, frame_assignment == s).sum()\n",
    "        union += np.logical_or(gt_assignment == s, frame_assignment == s).sum()\n",
    "    return intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79cd049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "436it [01:00,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4702)\n",
      "0.5681018721346461\n",
      "0.33076414795019843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drop_cost_type = 'logits'\n",
    "l2_normalize = False\n",
    "keep_percentile = 0.15\n",
    "framewise_acc = 0\n",
    "recall = 0\n",
    "iou = 0\n",
    "for idx, batch in tqdm(enumerate(valid_dl)):\n",
    "#     if idx % 100 == 0:\n",
    "#         print(recall/(idx+1))\n",
    "#         print(framewise_acc/(idx+1)) \n",
    "#         print(iou/(idx+1)) \n",
    "        \n",
    "    id_, step_len, step_features, video_len, video_features = batch['id'], batch['step_len'], batch['step_feature'], batch['video_len'], batch['video_feature']\n",
    "    \n",
    "    if drop_cost_type == 'learn':\n",
    "            distractors = torch.stack([ s[:size].mean(0) for s, size in zip(step_features, step_len)], dim=0).to(device) # also taking care of the distractor padding (dont worry about it later)\n",
    "    else:\n",
    "        distractors = [None] * batch_size\n",
    "\n",
    "    for _, sample in enumerate(zip(id_, step_len, step_features, video_len, video_features, distractors)):\n",
    "        _id, s_l, sf, v_l, vf, dif = sample\n",
    "        \n",
    "        if model is not None:\n",
    "            model.eval()\n",
    "            if drop_cost_type == 'learn':\n",
    "                dif = dif.to(device)\n",
    "            sf, vf, distractor = run_model_eval(vf.to(device), sf.to(device), dif, drop_cost_type)\n",
    "            \n",
    "            sf, vf, distractor = sf.detach().cpu(), vf.detach().cpu(), distractor\n",
    "            \n",
    "            if drop_cost_type == 'learn':\n",
    "                distractor = distractor.detach().cpu()\n",
    "            \n",
    "        else:\n",
    "#             need this to test no model baseline\n",
    "            sf, vf, distractor = sf, vf, dif\n",
    "\n",
    "        sim = sf[:s_l] @ vf[:v_l].T\n",
    "\n",
    "        zx_costs, drop_costs = compute_all_costs((sf, s_l, vf, v_l, distractor), l2_normalize=l2_normalize, gamma_xz=10, drop_cost_type=drop_cost_type, keep_percentile=keep_percentile)\n",
    "#         print(zx_costs.shape, drop_costs.shape)\n",
    "        zx_costs, drop_costs = [t.detach().cpu().numpy() for t in [zx_costs, drop_costs]]\n",
    "        sim = sim.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimal_assignment = drop_dtw(zx_costs, drop_costs, return_labels=True) - 1\n",
    "#         print(optimal_assignment)\n",
    "#         print(gt_training[_id])\n",
    "        recall += framewise_accuracy(optimal_assignment, gt_validation[_id], v_l, use_unlabeled=False)\n",
    "        framewise_acc += framewise_accuracy(optimal_assignment, gt_validation[_id], v_l, use_unlabeled=True).item()\n",
    "        iou += IoU(optimal_assignment, gt_validation[_id], s_l).item()\n",
    "#         optimal_assignment\n",
    "\n",
    "#         simple_assignment = np.argmax(sim, axis=0)\n",
    "#         simple_assignment[drop_costs < zx_costs.min(0)] = -1\n",
    "        \n",
    "print(recall/len(valid_dataset))\n",
    "print(framewise_acc / len(valid_dataset))\n",
    "print(iou / len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd166f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch):\n",
    "    framewise_acc = 0.\n",
    "    iou = 0.\n",
    "    \n",
    "    id_, step_len, step_features, video_len, video_features = batch['id'], batch['step_len'], batch['step_feature'], batch['video_len'], batch['video_feature']\n",
    "    \n",
    "    if drop_cost_type == 'learn':\n",
    "        distractors = torch.stack([ s[:size].mean(0) for s, size in zip(step_features, step_len)], dim=0).to(device)\n",
    "    else:\n",
    "        distractors = [None] * len(id_)\n",
    "    \n",
    "    for _, sample in zip(id_, step_len, step_features, video_len, video_features, distractors):\n",
    "        \n",
    "        _id, s_l, sf, v_l, vf, dis = sample\n",
    "        \n",
    "        if model is not None:\n",
    "            sf = sf.to(device)\n",
    "            vf = vf.to(device)\n",
    "            if dis is not None:\n",
    "                dis = dis.to(device)\n",
    "            m_sf, m_vf, m_dis = run_model_eval(vf, sf, dis)\n",
    "            m_sf, m_vf = m_sf.detach().cpu().numpy(), m_vf.detach().cpu().numpy()\n",
    "            if dis is not None:\n",
    "                m_dis = m_dis.detach().cpu().numpy()\n",
    "            \n",
    "        else:\n",
    "            m_sf, m_vf, m_dis = sf, vf, dis\n",
    "        \n",
    "        zx_costs, drop_costs = compute_all_costs((m_sf, s_l, m_vf, v_l, m_dis), l2_normalize=l2_normalize, gamma_xz=10, drop_cost_type=drop_cost_type, keep_percentile=keep_percentile)\n",
    "        zx_costs, drop_costs = [t.detach().cpu().numpy() for t in [zx_costs, drop_costs]]\n",
    "        optimal_assignment = drop_dtw(zx_costs, drop_costs, return_labels=True) - 1\n",
    "        \n",
    "        framewise_acc += framewise_accuracy(optimal_assignment, gt_validation[_id], v_l).item()\n",
    "        iou += IoU(optimal_assignment, gt_validation[_id], s_l).item()\n",
    "    return framewise_acc, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3a140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65106dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44668f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
