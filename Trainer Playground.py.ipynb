{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb914653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c062ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "\n",
    "from dtw import drop_dtw\n",
    "from data_handlers import YCDataset, SampleBatchIdx\n",
    "from models import EmbeddingsMapping\n",
    "from losses import compute_all_costs, compute_clust_loss, compute_alignment_loss\n",
    "from utils import compute_normalization_parameters\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, log, exp\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "opj = lambda x, y: os.path.join(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488a087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1237, 9), (436, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_csv('training_with_labels_s3dg.csv')\n",
    "validation_df = pd.read_csv('validation_with_labels_s3dg.csv')\n",
    "\n",
    "gt_training = torch.load('s3d_labelled_video_train.pkl')\n",
    "gt_validation = torch.load('s3d_labelled_video_val.pkl')\n",
    "training_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019299db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YCDataset(training_df, video_len=775)\n",
    "batch_sampler = SampleBatchIdx(train_dataset, 8, 24)\n",
    "train_dl = DataLoader(train_dataset, batch_sampler = batch_sampler)\n",
    "\n",
    "valid_dataset = YCDataset(validation_df, video_len=775)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad48adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"dropdtw\", entity=\"dhruvmetha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a938db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "epoch_curr = 0\n",
    "epochs = 10\n",
    "model = EmbeddingsMapping(512, video_layers=3, text_layers=3, drop_layers=2, learnable_drop=True, normalization_dataset=train_dataset, batch_norm=True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-5, steps_per_epoch=49, epochs=epochs)\n",
    "folder_name = 's3d_our_ckpts_notext_s3dg_cl4_dtw2.5_learn_adamw_lr_1cr_1e-5'\n",
    "model_name = 'best_model_state_25.pth'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "elif not os.path.exists(opj(folder_name, model_name)):\n",
    "    pass\n",
    "else:\n",
    "    model_state = torch.load(f'{folder_name}/best_model_state_5.pth')\n",
    "    model.load_state_dict(model_state['model'])\n",
    "    optimizer.load_state_dict(model_state['optimizer'])\n",
    "    epoch_curr = model_state['epoch']\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93876071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "l2_normalize = False\n",
    "drop_cost_type = 'learn'\n",
    "keep_percentile = 0.3\n",
    "\n",
    "clust_losses = []\n",
    "dtw_losses = []\n",
    "\n",
    "def run_model(vf, sf, distractor, drop_cost_type):\n",
    "    frame_features = model.map_video(vf)\n",
    "    step_features = model.map_text(sf)\n",
    "    if drop_cost_type == 'learn':\n",
    "        distractor_features = model.compute_distractors(distractor)\n",
    "    else:\n",
    "        distractor_features = [None] * frame_features.shape[0] ## bugs - check please\n",
    "    return step_features, frame_features, distractor_features\n",
    "\n",
    "def run_model_eval(vf, sf, distractor, drop_cost_type):\n",
    "    with torch.no_grad():\n",
    "        frame_features = model.map_video(vf)\n",
    "        step_features = model.map_text(sf)\n",
    "        if drop_cost_type == 'learn':\n",
    "            distractor_features = model.compute_distractors(distractor)\n",
    "        else:\n",
    "            distractor_features = [None] * frame_features.shape[0]\n",
    "        return step_features, frame_features, distractor_features\n",
    "    \n",
    "def framewise_accuracy(frame_assignment, gt_assignment, num_frames, use_unlabeled=False):\n",
    "    # to discount unlabeled frames in gt\n",
    "    if not use_unlabeled:\n",
    "        unlabled = np.count_nonzero(gt_assignment == -1)\n",
    "        num_frames = num_frames - unlabled\n",
    "        fa = np.logical_and(frame_assignment == gt_assignment, gt_assignment != -1).sum()\n",
    "    else:\n",
    "        fa = np.count_nonzero((frame_assignment == gt_assignment))\n",
    "    # framewise accuracy\n",
    "    fa = fa / num_frames if num_frames != 0 else 0\n",
    "    return fa\n",
    "\n",
    "def IoU(frame_assignment, gt_assignment, num_steps):\n",
    "\n",
    "    intersection, union = 0, 0\n",
    "    for s in range(num_steps):\n",
    "        intersection += np.logical_and(gt_assignment == s, frame_assignment == s).sum()\n",
    "        union += np.logical_or(gt_assignment == s, frame_assignment == s).sum()\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def evaluate(batch, drop_cost_type):\n",
    "    framewise_acc = 0.\n",
    "    iou = 0.\n",
    "    \n",
    "    id_, step_len, step_features, video_len, video_features = batch['id'], batch['step_len'], batch['step_feature'], batch['video_len'], batch['video_feature']\n",
    "#     print(id_, step_len, step_features, video_len, video_features)\n",
    "    \n",
    "    if drop_cost_type == 'learn':\n",
    "        distractors = torch.stack([ s[:size].mean(0) for s, size in zip(step_features, step_len)], dim=0).to(device)\n",
    "    else:\n",
    "        distractors = [None] * len(id_)\n",
    "    \n",
    "    for _, sample in enumerate(zip(id_, step_len, step_features, video_len, video_features, distractors)):\n",
    "        \n",
    "        _id, s_l, sf, v_l, vf, dis = sample\n",
    "        with torch.no_grad():\n",
    "            if model is not None:\n",
    "                sf = sf.to(device)\n",
    "                vf = vf.to(device)\n",
    "                if dis is not None:\n",
    "                    dis = dis.to(device)\n",
    "                m_sf, m_vf, m_dis = run_model_eval(vf, sf, dis, drop_cost_type)\n",
    "                m_sf, m_vf = m_sf.detach().cpu().numpy(), m_vf.detach().cpu().numpy()\n",
    "                if dis is not None:\n",
    "                    m_dis = m_dis.detach().cpu().numpy()\n",
    "\n",
    "            else:\n",
    "                m_sf, m_vf, m_dis = sf, vf, dis\n",
    "\n",
    "            \n",
    "            zx_costs, drop_costs = compute_all_costs((m_sf, s_l, m_vf, v_l, m_dis), l2_normalize=l2_normalize, gamma_xz=10, drop_cost_type=drop_cost_type, keep_percentile=keep_percentile)\n",
    "            zx_costs, drop_costs = [t.detach().cpu().numpy() for t in [zx_costs, drop_costs]]\n",
    "            optimal_assignment = drop_dtw(zx_costs, drop_costs, return_labels=True) - 1\n",
    "\n",
    "            framewise_acc += framewise_accuracy(optimal_assignment, gt_validation[_id], v_l).item()\n",
    "#             iou += IoU(optimal_assignment, gt_validation[_id], s_l).item()\n",
    "    return framewise_acc, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2c0f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [02:02<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t  Loss: 5.793319141163545 Frame Acc: 0.5899965973878536 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:51<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t  Loss: 4.728760766048057 Frame Acc: 0.5898080412725654 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:55<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \t  Loss: 4.004155074848848 Frame Acc: 0.5872949571733218 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t  Loss: 3.355844965168074 Frame Acc: 0.5850236794885693 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:56<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t  Loss: 2.844718914405972 Frame Acc: 0.5836069938649825 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \t  Loss: 2.460940496594298 Frame Acc: 0.5840526704736259 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:55<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \t  Loss: 2.179210106531779 Frame Acc: 0.5854672565372712 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:55<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \t  Loss: 1.8782471371631997 Frame Acc: 0.5848227989851335 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:55<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \t  Loss: 1.64111889109892 Frame Acc: 0.584742695588721 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:54<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \t  Loss: 1.4525970173817055 Frame Acc: 0.5849216485549824 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t  Loss: 1.253883869040246 Frame Acc: 0.5838135846745257 IoU: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:56<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "l2_normalize = False\n",
    "drop_cost_type = 'learn'\n",
    "keep_percentile = 0.3\n",
    "\n",
    "clust_losses = []\n",
    "dtw_losses = []\n",
    "\n",
    "prev_loss = 100000.0\n",
    "for epoch in range(epoch_curr, epochs):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        step_len, step_features, video_len, video_features = batch['step_len'], batch['step_feature'], batch['video_len'], batch['video_feature']\n",
    "        \n",
    "        if drop_cost_type == 'learn':\n",
    "            distractors = torch.stack([s[:size].mean(0) for s, size in zip(step_features, step_len)], dim=0).to(device) # also taking care of the distractor padding (dont worry about it later)\n",
    "        else:\n",
    "            distractors = None\n",
    "            \n",
    "        sf, ff, dif = run_model(video_features.to(device), step_features.to(device), distractors, drop_cost_type) # adding all the features here to gpu\n",
    "        \n",
    "        sample = (sf, step_len, ff, video_len, dif)\n",
    "        clust_loss = compute_clust_loss(sample, xz_gamma=30, frame_gamma=10, l2_normalize=l2_normalize, device=device)\n",
    "        \n",
    "        dtw_loss = compute_alignment_loss(sample, drop_cost_type, gamma_xz=10, gamma_min=1, keep_percentile=keep_percentile, l2_normalize=l2_normalize, device=device)\n",
    "        \n",
    "        clust_losses.append(clust_loss.item())\n",
    "        dtw_losses.append(dtw_loss.item())\n",
    "\n",
    "        total_loss = dtw_loss\n",
    "#         print((4 * clust_loss), (2.5 * dtw_loss))\n",
    "        loss += total_loss.item()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "    loss /= 51.0\n",
    "#     print(clust_losses)\n",
    "    model.eval()\n",
    "    frame_acc, iou_acc = evaluate(next(iter(valid_dl)), drop_cost_type)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, f'{folder_name}/best_model_state_{epoch+1}.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch} \\t  Loss: {loss} Frame Acc: {frame_acc/len(valid_dataset)} IoU: {iou_acc/len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag([10] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfb1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed22596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa861ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ef230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e379a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec2e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03176017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
